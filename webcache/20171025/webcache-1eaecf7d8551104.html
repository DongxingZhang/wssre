<div class="blk_container">
    <p> 热点聚焦:10月19日,nature 杂志上线了DeepMind 团队的最新研究成果:新一代的AlphaGo Zero
        完全从零开始,不需要任何历史棋谱的指引,通过自我对弈490万棋局后,以100:0的成绩击败了上一版的AlphaGo。<br/><br/>
        简评:本次的AlphaGo Zero 相比之前版本的AlphaGo
        除了将策略网络与价值网络一起训练共享参数,同时简化了蒙特卡洛树搜索之外,一个非常重要的变化是不再让机器先经过“打谱”进行有监督学习,而是完全从零开始通过自我对弈的强化学习就达到了更好的效果,而这个效果仅仅用了490万的自我对弈,相比围棋高达10170的状态空间复杂度而言无疑是沧海一粟。一个可能的解释是首先CNN(卷积神经网络)的模型适用范围非常适合围棋的规则,能得到非常完美的表达;其次之前借助人类棋谱的有监督学习或许一定程度上“误导”了机器的思路,导向了局部最优解而不是全局最优解;另外强化学习在规则清晰的随机系统序列决策问题中有望获得超过人类经验的效果,拓宽认知的边界。<br/><br/>
        强化学习是机器学习的一种,通过与“环境”交互逐步进行学习。在交互过程中,机器的“行动”与所面对的“环境”互相影响,而且每一个行为带来的后果会有一个延迟。因此强化学习是针对过程的决策,追求的是全局最优解。比如一局机器最终取胜的围棋比赛中,到底是其中的一次妙手,还是之前的伏笔抑或之后看似不经意的某一步带领机器获胜几乎是无法判断的。在这种情形下,人类是无法给某一步贴上对或错的标签。只能在每次对弈后根据胜负结果给予机器代理不同程度的奖励,而机器代理所要做的则是努力让自己每盘棋积累的奖励最大化。<br/><br/>
        强化学习有望在在自动驾驶汽车、飞行器控制、机器人等领域取得很好的应用效果。以自动驾驶汽车为例,自动驾驶过程可以分为感知、决策和控制三大部分,目前机器视觉等已经能较好的解决感知过程的问题,而感知过程完成之后如何根据感知的结果作出合理的决策无疑至关重要。而强化学习通过与“环境”的交互学习进而进行决策,能够根据变化的“环境”采取每步获取最大价值的策略,有望取得很好的应用效果。<br/><br/>
        创业之星:中科虹霸,领跑虹膜识别行业中科虹霸成立于2006年12月,由中国科学院自动化研究所发起设立,从事生物特征识别领域的技术研究、产品开发与成果转化,致力于为客户提供基于虹膜识别的身份认证平台。目前,公司的虹膜识别产品已广泛应用于金融、军事、公安安全、移动终端、矿山等众多领域,取得了良好的应用效果。公司是国际上少数拥有虹膜识别核心技术专利的企业之一,掌握具有完全自主知识产权虹膜识别技术。随着三星推出虹膜识别功能后,其他智能手机巨头有望相继入局,消费电子虹膜市场未来五年CAGR
        约为35.18%,公司将充分受益。<br/><br/>
        风险提示:<br/><br/>
        计算能力提升不及预期的风险;模型算法改进缓慢的风险;平台发展缓慢的风险。</p>
</div>