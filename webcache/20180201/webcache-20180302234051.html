<div class="blk_container">
    <p> 人工智能算法存在的问题与传统统计方法的理论优势。<br/><br/>
        AlphaGo Zero
        的成功,让“人工智能+”的概念深入人心。复杂度较高的数据挖掘算法有效的前提是问题一定有确定解(即使模型范式非常复杂),同时要求数据的“质”和“量”达到一定要求。在量化投资应用中,数据质量一般(共线性、滞后性、信噪比低等)、数据量不够大(宏观经济数据等)的时候,强行套用黑箱模型,不仅模型的解释性差、参数敏感,而且非常容易出现过拟合。针对基本面的数据,具有强有力理论支撑的传统统计方法依然表现出了较好的性能。基于以上,我们推出温故知新系列专题,重新梳理部分传统统计方法的理论基础和适用范围。<br/><br/>
        “偏差―方差”模型分解OLS 回归方法估计的误差来源。<br/><br/>
        线性回归因自变量共线性、实际分布厚尾、存在离群点等问题,OLS
        回归预测总误差较大。本文基于针对预测总误差的“偏差―方差”分解,分析了估计误差来源。介绍了弹性网族回归(Lasso、ENet、Ridge)、非凸惩罚函数回归(SCAD、MCP)、分位数回归的差异与效果,通过控制模型方差和偏差,最终降低模型预测总误差,相对于OLS
        回归,显著提升变量选择能力和预测的稳健性。<br/><br/>
        Lasso 目标函数为凸易计算,压缩无关变量系数为0,鲁棒性佳Ridge
        回归唯一有显示解,计算简单;ENet、Lasso、SCAD、MCP回归均能将较小系数压缩至0,且选择性压缩共线性变量中的一个。Lasso、SCAD、MCP 回归方法的变量选择最有效,样本外的预测效果最佳。Lasso
        目标函数为凸易计算,压缩无关变量系数为0,鲁棒性佳,尤其实用。SCAD
        满足渐近无偏性,但计算复杂。本文针对样本数量为100和1000的数据进行了数值模型,比较了不同方法的变量选择能力、拟合效果和估计误差。<br/><br/>
        分位数回归忽略残差假设,多条回归曲线提供更多信息。<br/><br/>
        分位数回归不考虑同方差、正态的假设,具备异常点耐抗性,捕捉分布尾部特征等特点,比OLS 回归更稳健;不仅仅分析被解释变量的条件期望,亦可分析被解释变量的中位数、分位数情况。<br/><br/>
        应用实例:PPI
        和海外利率是近期国内长债利率上行直接影响因素不同时期长债利率的直接影响因子不同。2008-2012年国内核心影响变量是经济,2013年的钱荒直接基本面因素影响较小,2014年之后利率更多受到海外利率的影响。本文以Lasso
        回归为例,我们滚动计算了经济、通胀及国外利率和十年期国债收益率月度环差48个月数据对国内长债利率的影响。</p>
</div>